from keras.preprocessing.sequence import pad_sequences
import numpy as np
from numpy.random import rand, randint, choice

from shcomplete.model2predict import Vocabulary, generate_model, train


def get_chars(path_to_vocab):
    """
    Return the list of all characters found in the vocabulary.
    """
    chars = set()
    with open(path_to_vocab) as f:
        for line in f:
            chars.update(line.rstrip())
    return chars


def create_mistakes(chars, command, level_noise):
    """
    Add artificial spelling mistakes into a single command.
    The number of mistakes is proportional to the level of noise we choose.
    """
    MISTAKE_SWAP = "swap"
    MISTAKE_DELETION = "deletion"
    MISTAKE_ADDITION = "addition"
    MISTAKE_TRANSPOSITION = "transposition"
    mistakes = [MISTAKE_SWAP, MISTAKE_ADDITION, MISTAKE_DELETION, MISTAKE_TRANSPOSITION]
    ind = randint(len(mistakes))
    chars = list(chars)

    if command is not None and rand() < level_noise:
        char_pos = randint(len(command))
        if mistakes[ind] == MISTAKE_SWAP:
            command = command[:char_pos] + choice(chars) + command[char_pos + 1:]
        elif mistakes[ind] == MISTAKE_DELETION:
            command = command[:char_pos] + command[char_pos + 1:]
        elif mistakes[ind] == MISTAKE_ADDITION:
            command = command[:char_pos] + choice(chars) + command[char_pos:]
        else:
            assert mistakes[ind] == MISTAKE_TRANSPOSITION
            char_pos = randint(len(command) - 1)
            command = (command[:char_pos] + command[char_pos + 1] + command[char_pos] +
                       command[char_pos + 2:])
    return command


class Seq2seq():
    def __init__(self, chars):
        self.chars = sorted(chars)
        self.indices_char = {}
        self.char_indices = {}
        for i, char in enumerate(self.chars):
            self.indices_char[i + 1] = char
            self.char_indices[char] = i + 1

    def encode(self, commands):
        """
        Encode a command line into a numpy array.
        """
        commands_encoded = [[] for _ in range(len(commands))]
        for i, cmd in enumerate(commands):
            for char in cmd:
                commands_encoded[i].append(self.char_indices[char])
        return commands_encoded

    def decode(self, X, reduction=True, inverted=False):
        """
        Decode the numpy array X and return the corresponding command.
        """
        command = ""
        if reduction:
            if inverted:
                begin_of_command = np.amin(np.nonzero(X))
                for i in range(begin_of_command, X.shape[0]):
                    command += self.indices_char[X[i]]
            else:
                X = np.squeeze(X)
                end_of_command = np.amax(np.nonzero(X))
                for i in range(end_of_command + 1):
                    command += self.indices_char[X[i]]
            return command
        else:
            try:
                end_of_command = np.amax(np.nonzero(X))
            except ValueError:
                end_of_command = -1
            for i in range(end_of_command + 1):
                try:
                    command += self.indices_char[X[i]]
                except KeyError:
                    pass
        return command


def generator_misprints(args):
    """
    Generate a tuple (misspelled_command, true_command) to train the model.
    The misspelled_command is generated by adding artificial mistakes.abs
    Padding is added to feed the model with fixed length sequences of characters.
    An epoch finishes when samples_per_epoch samples have been seen by the model.
    """
    chars = get_chars(args.vocabulary)
    seq2seq = Seq2seq(chars)
    vocab = Vocabulary(args.vocabulary)
    vocab_trie = vocab.trie(args.vocabulary)
    with open(args.corpus) as f:
        lines = np.array([cmd.strip() for cmd in f if cmd.strip()])
        nb_lines = lines.shape[0]
        id_lines = np.arange(nb_lines)
        while True:
            commands = []
            misspelled_commands = []
            while len(commands) < args.batch_size:
                command = lines[np.searchsorted(id_lines, randint(nb_lines - 1))]
                prefix = vocab_trie.longest_prefix(command)[0]
                if prefix is not None:
                    misspelled_command = create_mistakes(chars, prefix, args.level_noise)
                    commands.append(prefix)
                    misspelled_commands.append(misspelled_command[::-1])
            misspelled_commands_encoded = seq2seq.encode(misspelled_commands)
            commands_encoded = seq2seq.encode(commands)
            X = pad_sequences(misspelled_commands_encoded, maxlen=args.max_cmd_len,
                              dtype="int32", padding="pre", truncating="pre")
            y = pad_sequences(commands_encoded, maxlen=args.max_cmd_len,
                              dtype="int32", padding="post", truncating="post")
            yield X, y[:, :, np.newaxis]


def dislpay_sample_correction(args, model, X, y, nb_corrections=10):
    """
    Select a given number of misspelled commands and print the current correction of the model.
    """
    chars = get_chars(args.vocabulary)
    seq2seq = Seq2seq(chars)
    print()
    for _ in range(nb_corrections):
        ind = choice(X.shape[0])
        rowX = X[np.array([ind])]
        rowy = y[np.array([ind])]
        preds = model.predict_classes(rowX)
        true_command = seq2seq.decode(rowy[0])
        model_correction = seq2seq.decode(preds[0], reduction=False)
        misspelled_command = seq2seq.decode(rowX[0], inverted=True)
        print('Command misspelled :', misspelled_command[::-1])
        print('True command :', true_command)
        print("Correction predicted :", model_correction)
        print('---')
    return model_correction


def initialize_model2correct(args):
    """
    Initialize the model and the stack of layers with the right dimensions.
    The architecture of the layers is specific to the misprints correction problem.
    """
    chars = get_chars(args.vocabulary)
    nb_features = len(chars) + 1
    model = generate_model(args, nb_features, input_length=args.max_cmd_len,
                           nb_repeats=args.max_cmd_len)
    return model


def train_correct(args):
    """
    Train a RNN to correct misprints in command lines.
    At each epoch, some misprint corrections are displayed with training statistics.
    """
    train(args, initialize_model2correct, generator_misprints,
          dislpay_sample_correction)
