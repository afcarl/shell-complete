import logging
import os

from keras.callbacks import Callback
from keras.layers import Dense, RepeatVector, Dropout, TimeDistributed, Activation, recurrent
from keras.layers.embeddings import Embedding
from keras.models import Sequential, load_model
from keras.preprocessing.sequence import pad_sequences
import numpy as np
from numpy.random import rand, randint, choice

from shcomplete.model2predict import Vocabulary


def get_chars(path_to_vocab):
    """
    Return the list of all characters found in the vocabulary.
    """
    chars = set()
    with open(path_to_vocab) as f:
        for line in f:
            chars.update(line.rstrip())
    return chars


def create_mistakes(chars, command, level_noise):
    """
    Add artificial spelling mistakes into a single command.
    The number of mistakes is proportional to the level of noise we choose.
    """
    mistakes = {0: "swap", 1: "deletion", 2: "addition", 3: "transposition"}
    ind = randint(len(mistakes))
    chars = list(chars)

    if command is not None and rand() < level_noise:
        char_pos = randint(len(command))
        if mistakes[ind] == "swap":
            command = command[:char_pos] + choice(chars) + command[char_pos + 1:]
        elif mistakes[ind] == "deletion":
            command = command[:char_pos] + command[char_pos + 1:]
        elif mistakes[ind] == "addition":
            command = command[:char_pos] + choice(chars) + command[char_pos:]
        else:
            assert mistakes[ind] == "transposition"
            char_pos = randint(len(command) - 1)
            command = (command[:char_pos] + command[char_pos + 1] + command[char_pos] +
                       command[char_pos + 2:])
    return command


class Seq2seq(object):
    def __init__(self, chars):
        self.chars = sorted(chars)
        self.indices_char = {}
        self.char_indices = {}
        for i in range(len(self.chars)):
            self.indices_char[i + 1] = self.chars[i]
            self.char_indices[self.chars[i]] = i + 1

    def encode(self, commands):
        commands_encoded = [[] for _ in range(len(commands))]
        for i, cmd in enumerate(commands):
            for char in cmd:
                commands_encoded[i].append(self.char_indices[char])
        return commands_encoded

    def decode(self, X, reduction=True, inverted=False):
        """
        Decode the numpy array X and return the corresponding command.
        """
        command = ""
        if reduction:
            if inverted:
                begin_of_command = np.amin(np.nonzero(X))
                for i in range(begin_of_command, X.shape[0]):
                    command += self.indices_char[X[i]]
            else:
                X = np.squeeze(X)
                end_of_command = np.amax(np.nonzero(X))
                for i in range(end_of_command + 1):
                    command += self.indices_char[X[i]]
            return command
        else:
            try:
                end_of_command = np.amax(np.nonzero(X))
            except ValueError:
                end_of_command = -1
            for i in range(end_of_command + 1):
                try:
                    command += self.indices_char[X[i]]
                except KeyError:
                    pass
        return command


def generator(chars, vocabulary, corpus, max_cmd_len, batch_size, level_noise):
    """
    Generate a tuple (misspelled_command, true_command) to train the model.
    The misspelled_command is generated by adding artificial mistakes.abs
    Padding is added to feed the model with fixed length sequences of characters.
    An epoch finishes when samples_per_epoch samples have been seen by the model.
    """
    seq2seq = Seq2seq(chars)
    vocab = Vocabulary(vocabulary)
    vocab_trie = vocab.trie(vocabulary)
    with open(corpus) as f:
        lines = np.array([cmd.strip() for cmd in f if cmd != "\n"])
        nb_lines = lines.shape[0]
        id_lines = np.arange(nb_lines)
        while True:
            commands = []
            misspelled_commands = []
            while len(commands) < batch_size:
                command = lines[np.searchsorted(id_lines, randint(nb_lines - 1))]
                prefix = vocab_trie.longest_prefix(command)[0]
                if prefix is not None:
                    misspelled_command = create_mistakes(chars, prefix, level_noise)
                    commands.append(prefix)
                    misspelled_commands.append(misspelled_command[::-1])
            misspelled_commands_encoded = seq2seq.encode(misspelled_commands)
            commands_encoded = seq2seq.encode(commands)
            X = pad_sequences(misspelled_commands_encoded, maxlen=max_cmd_len,
                              dtype="int32", padding="pre", truncating="pre")
            y = pad_sequences(commands_encoded, maxlen=max_cmd_len,
                              dtype="int32", padding="post", truncating="post")
            yield X, y[:, :, np.newaxis]


def generate_model(chars, args):
    """
    Generate the model.
    """
    nb_chars = len(chars) + 1
    emb_weights = np.eye(nb_chars)

    model = Sequential()
    model.add(Embedding(input_dim=nb_chars, output_dim=nb_chars, input_length=args.max_cmd_len,
                        weights=[emb_weights], trainable=False))
    for layer_id in range(args.input_layers):
        model.add(recurrent.LSTM(args.hidden_layers,
                                 return_sequences=layer_id + 1 < args.input_layers))
        model.add(Dropout(args.dropout))

    model.add(RepeatVector(args.max_cmd_len))
    for _ in range(args.output_layers):
        model.add(recurrent.LSTM(args.hidden_layers, return_sequences=True))
        model.add(Dropout(args.dropout))

    model.add(TimeDistributed(Dense(nb_chars)))
    model.add(Activation("softmax"))
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer="adam",
                  metrics=["accuracy"])
    return model


def sample_prediction(model, chars, X, y, nb_predictions):
    """
    Select 10 misspelled commands and print the current correction of the model.
    """
    seq2seq = Seq2seq(chars)
    print()
    for _ in range(nb_predictions):
        ind = choice(X.shape[0])
        rowX = X[np.array([ind])]
        rowy = y[np.array([ind])]
        print("shapeeeee :", rowX.shape)
        preds = model.predict_classes(rowX)
        print("predssss :", preds.shape)
        true_command = seq2seq.decode(rowy[0])
        model_correction = seq2seq.decode(preds[0], reduction=False)
        misspelled_command = seq2seq.decode(rowX[0], inverted=True)
        print('Command misspelled :', misspelled_command[::-1])
        print('True command :', true_command)
        print("Correction predicted :", model_correction)
        print('---')
    return model_correction


class OnEpochEndCallback(Callback):
    def __init__(self, args, log):
        self.log = log
        self.vocabulary = args.vocabulary
        self.corpus = args.corpus
        self.models_directory = args.model_directory
        self.max_cmd_len = args.max_cmd_len
        self.batch_size = args.batch_size
        self.level_noise = args.level_noise
        self.nb_predictions = args.nb_predictions
        self.checkpoint = args.checkpoint
        self.model_directory = args.model_directory

    def on_epoch_end(self, epoch, logs=None):
        """
        Apply the prediction of the model to a batch of data at the end of each epoch.
        """
        chars = get_chars(self.vocabulary)
        X, y = next(generator(chars, self.vocabulary, self.corpus, self.max_cmd_len,
                              self.batch_size, self.level_noise))
        sample_prediction(self.model, chars, X, y, self.nb_predictions)
        path_to_model = os.path.join(self.model_directory, "keras_spell_e{}.h5".format(epoch))
        if epoch % self.checkpoint == 0:
            self.log.info("Saving the model to %s", path_to_model)
            self.model.save(path_to_model)


def train_correct(args, log_level=logging.INFO):
    """
    Train the model and show the progress of the prediction at each epoch.
    """
    _log = logging.getLogger("training")
    _log.setLevel(log_level)
    chars = get_chars(args.vocabulary)

    if args.from_model:
        model = load_model(args.from_model)
    else:
        model = generate_model(chars, args)

    ON_EPOCH_END_CALLBACK = OnEpochEndCallback(args, _log)
    model.fit_generator(generator(chars, args.vocabulary, args.corpus, args.max_cmd_len,
                                  args.batch_size, args.level_noise),
                        samples_per_epoch=args.steps_per_epoch,
                        nb_epoch=args.nb_epochs,
                        callbacks=[ON_EPOCH_END_CALLBACK, ],
                        validation_data=None)
